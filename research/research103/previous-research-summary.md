# Previous Research Summary (Last 10 Iterations)

## Key Pattern: 100% Consensus on All Recent Questions

The last 10+ questions have ALL achieved 100% consensus, meaning LLMs correctly answered them. This indicates that modern frontier models are now solving:
- Letter counting questions (ELEVEN ELEPHANTS - 5 E's)
- Self-referential questions (word count if answer is 'five' = 1)
- Classic riddles (coins puzzle, farmer's sheep)
- Alphabetical ordering questions
- Basket/subtraction word problems

## Research 102 Findings (Most Recent)

**Question Tested**: "How many words are in your answer to this question if you answer with exactly the word 'five'?"
**Answer**: 1
**Score**: 100% consensus (all LLMs got it right)

**Key Research from 102**:
1. Anthropic's Inverse Scaling (2025) - more reasoning can hurt performance
2. LLM Rightward Spatial Deficiency - asymmetric spatial reasoning
3. Word Count Constraint Failures - LLMs can't count their own output
4. CoT Reasoning Interferes with Instruction Following
5. Multimodal LLMs are Shape-Blind

**Conclusion from 102**: Semantic-metalinguistic conflict questions (word meaning vs. word count) are now SOLVED by frontier models.

## Patterns That No Longer Work (All Hitting 100%)

1. **Letter counting**: Even with semantic interference (ELEVEN = 6 letters)
2. **Self-reference about answers**: "How many words in your answer if..."
3. **Classic riddles**: Farmer's sheep, coins, "all but X" constructions
4. **Alphabetical ordering**: Words sorted by spelling
5. **Simple math word problems**: Basket with apples, taking items
6. **Constraint satisfaction**: "Answer yes or no" style questions

## What Has Been Explored (100+ Iterations)

Looking back across iterations 48-102:
- Syllable counting (works for LLMs now)
- Last/first letter questions (solved)
- Position/sequence in lists (solved)
- Self-referential paradoxes (WRONG_DIRECTION - not easy for humans)
- Physical intuition (water in tilted glass, etc. - solved)
- Mirror/perspective questions (solved)
- Left/right spatial (solved)
- Homophones (one/won/sun sounds - solved)
- Overlapping letter pairs (11 in 11111 - solved)
- Decimal comparison ($9.9 vs $9.11 - solved)
- Ordinal word positions (solved)
- Letter counting in words (solved)
- "Can you spell X using only letters Y" (solved)

## The Challenge

After 100+ iterations, frontier models in 2026 are solving ALL simple questions that can be decomposed into enumerable steps. The remaining challenge is finding questions that:

1. Are trivially easy for humans (5-year-old test)
2. Cannot be solved by step-by-step enumeration
3. Exploit TRUE architectural limitations, not just training gaps
4. Are genuinely simple (not trick questions or paradoxes)

## Potential Unexplored Areas

1. **Questions where enumeration LEADS to wrong answers** (inverse scaling)
2. **Intuition-based questions with no computation path**
3. **Gestalt perception** (seeing patterns holistically, not component-wise)
4. **Time/sequence questions with misdirection**
5. **Pragmatic inference failures** (what's implied vs. what's stated)
