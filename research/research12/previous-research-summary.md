# Previous Research Summary for Iteration 12

## Completed Iterations (11 total)

| Iteration | Question | Score | Key Learning |
|-----------|----------|-------|--------------|
| 1 | Python ceil() math | 100% | Technical questions solved |
| 2 | Last letter of 'FIRST' | 100% | Semantic traps caught |
| 3 | FOUR vs FIVE letters | 100% | Enumeration defeats traps |
| 4 | Third 'E' in ELEVEN | 100% | Position tracking works |
| 5 | Letters in 'FIVE LETTERS' | 100% | Self-referential claims ignored |
| 6 | Girl with 3 brothers | 80% | **PARTIAL** - relational reasoning |
| 7 | River crossing (swimming) | 100% | Explicit modifications caught |
| 8 | 4 daughters, brother no sisters | 80% | **PARTIAL** - contradiction detection |
| 9 | Store is closed | 100% | Override conditions caught |
| 10 | 2 lbs vs 2 kg | 100% | Unit mismatches caught |
| 11 | Race - pass second place | 100% | Classic cognitive traps solved |

## MASSIVE INSIGHT AFTER 11 ITERATIONS

**2026 frontier models have essentially ZERO blind spots for:**
- Any question that can be systematically verified
- Any well-documented cognitive trap
- Any explicit modification to famous puzzles
- Any question with numbers that need conversion/comparison

**The ONLY partial successes (80%) came from:**
- Complex relational reasoning (iteration 6)
- Contradiction detection where "solve mode" overrides "verify mode" (iteration 8)

## What MUST Be Different for Iteration 12

### AVOID COMPLETELY:
1. Letter/word counting
2. Famous puzzle modifications
3. Well-known cognitive traps (like the race question)
4. Questions with explicit conditions to catch
5. Unit conversions or comparisons
6. Anything documented in research papers

### EXPLORE:
1. **Questions that require GENERATING new information** - not verifying existing
2. **Questions about the model's own output** - self-referential in a new way
3. **Questions with genuine ambiguity** that humans resolve but LLMs can't
4. **Questions that exploit tokenization** - where the text representation differs from meaning
5. **Questions that require physical/spatial intuition** beyond simple logic

## Key Research Directions

### 1. Self-Referential Output Questions
- "Answer this question in exactly 5 words. How many words is your answer?"
- Forces the model to generate AND count simultaneously

### 2. Impossible Instruction Questions
- Questions that contradict themselves in subtle ways
- Where following the instruction makes the answer wrong

### 3. Physical Intuition Beyond Logic
- Questions where overthinking produces wrong answer
- Where simple intuition trumps analysis

### 4. Hidden Structure Questions
- Where the question's structure contains the answer
- But the structure is not obvious

### 5. Perspective/Context-Dependent Questions
- Where the answer depends on information the model should ask about
- But models typically don't ask clarifying questions
