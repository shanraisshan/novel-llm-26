# Previous Research Summary for Iteration 25

## Best Results So Far
- **Iteration 13 (20% consensus)**: Self-referential - BANNED
- **Iteration 12 (40% consensus)**: Self-referential - BANNED
- **Iteration 6 (80% consensus)**: Sibling relational reasoning
- **Iteration 8 (80% consensus)**: Contradiction detection

## Key Insights from 24 Iterations

### EVERYTHING FAILS AT 100% CONSENSUS:
After 24 iterations, EVERY non-self-referential approach has achieved 100% consensus:
1. Letter counting / semantic primes
2. Position tracking
3. Explicit puzzle modifications
4. Override conditions
5. Unit mismatches
6. Classic cognitive traps
7. Arithmetic shortcuts
8. Spatial reasoning
9. Physical world intuition
10. Irrelevant information traps
11. Overthinking traps
12. Age gap riddles
13. False presupposition rejection (math)
14. **Evaporation/physical state transformation (iteration 24)**

### The Only Partial Successes (80%):
- Sibling reasoning (iteration 6)
- Contradiction detection (iteration 8)

## CRITICAL ANALYSIS

After 24 failed iterations, the pattern is clear:
- 2026 frontier models are EXTREMELY robust
- All documented failure modes from research papers are patched
- All famous riddles/puzzles are in training data
- Simple physical reasoning works correctly
- Simple false presuppositions are rejected correctly
- Presuppositional language doesn't trick models

## What MIGHT Still Work

### Completely Unexplored Territories:
1. **Genuine linguistic ambiguity** - Not trick questions, but genuinely ambiguous sentences
2. **Garden path sentences** - Where initial parsing is wrong
3. **Scope ambiguity** - "Every man loves some woman"
4. **Pragmatic implicature** - What's IMPLIED but not stated
5. **Questions requiring REJECTION of common knowledge** - Where the "obvious" answer is wrong
6. **Questions about edge cases in language** - Unusual constructions

### MUST ABSOLUTELY AVOID:
- Self-referential (BANNED)
- Famous riddles/puzzles
- Documented failure modes
- Simple physics/math
- Semantic primes
- Famous cognitive traps
- Presuppositional traps (iteration 24 showed these don't work)

## The Real Challenge

Finding a question that:
1. Has ONE correct answer
2. Humans find trivially easy
3. LLMs consistently fail
4. Is NOT self-referential
5. Is NOT a known failure mode

...is proving nearly impossible with 2026 frontier models.
